{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying Data Science to DS Job Hunting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indeed API\n",
    "Indeed.com offers a publisher's API for adding links in a web page or app. I decided to use this API\n",
    "to gather a sample of job posting from which to scrape a list of skills.\n",
    "\n",
    "The API will only return a maximum of 25 url's, so one needs a trick to get a significant amount of data. The trick I'm using now, is to query by zipcode. There are ~43K in the US so that's going to hopefully bring us some hits.\n",
    "For now, I'm using 500 randomly selected samples of the ~43K zipcodes, returning from 0 to 25 urls from each.\n",
    "\n",
    "## Parsing Out Skills\n",
    "To parse out what I think are the skills, I use BeautifulSoup to iterate over the sections locating the bulleted points:\n",
    "<span class='summary'>\n",
    "<li> SQL\n",
    "<li> Python\n",
    "<li> AWS\n",
    "</span>\n",
    "\n",
    "Visual inspection indicates that most of the time, an employer will use a list to itemize the position skills.\n",
    "It would be cool to run a second supporting project that tries to verify this. How many job posting contain any itemized lists versus those that do not ?\n",
    " \n",
    "### Stop Words\n",
    "I wanted a way to add new stop words. The word \"data\" obviously shows up many times and is not helpful.\n",
    "\n",
    "## Begin Analysis\n",
    "### Bar Plot \n",
    "To count up the parsed skill tokens, I employ SciKit-Learn's CountVectorizer and produce a simple bar plot output.\n",
    "\n",
    "### Locations\n",
    "For this example, I'm using all the zipcodes that start with '9' and 100 randomly selected samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark import  SparkContext\n",
    "sc = SparkContext( 'local', 'pyspark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import indeed_scrape\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "rcParams['figure.figsize'] = (15, 8)\n",
    "\n",
    "%matplotlib inline\n",
    "ind = indeed_scrape.Indeed()\n",
    "ind.query = \"data science\"\n",
    "ind.add_loc = '90' # will add regex-ed zip codes\n",
    "ind.num_samp = 0\n",
    "ind.stop_words = \"director media relations\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ind.add_stop_words()\n",
    "ind.load_config()\n",
    "ind.build_api_string()\n",
    "locs = ind.handle_locations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "locsRDD = sc.parallelize(locs)\n",
    "url_city = locsRDD.flatMap(lambda x: ind.get_url(x))\n",
    "out = url_city.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df['url'] = [item[0] for item in out]\n",
    "df['city'] = [item[1] for item in out]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "skills = sc.parallelize(df['url'])\n",
    "s = skills.map(ind.parse_content)\n",
    "df['skill'] = s.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stem = sc.parallelize(df['skill'])\n",
    "st = stem.map(ind.tokenizer)\n",
    "df['skill_stem'] = st.collect()\n",
    "df.to_csv(\"/home/daniel/git/Python2.7/DataScience/indeed/data_frame_gary.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coming Back "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data_frame_gary.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at how many job postings were returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = df.drop_duplicates().dropna()\n",
    "df['url'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bi-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 8))\n",
    "matrix, features = ind.vectorizer(df['skill_stem'])\n",
    "ind.plot_features(features, matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monogram \n",
    "Above a Bi-gram analysis was performed by default. Let's include single words in the n-gram range,\n",
    "(1,2), and using a corpus that has been stemmed with NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus_stem = df['skill_stem']\n",
    "mat, fea = ind.vectorizer(corpus_stem, n_min=1)\n",
    "plt.figure(figsize=(15,8))\n",
    "ind.plot_features(fea, mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore High Count Words \n",
    "The word \"experience\" showed up with a high count. I want to know if there's more to that. Experience with a platform, technology, SQL or jusy previous analytic experiece. NLP is a deep rabbit hole, and I only peered a short ways down for this project. \n",
    "\n",
    "My word radius method gathers words to the left and right of a chosen keyword, and builds a corpus from within that radius. Then I apply the CountVectorizer again.\n",
    "\n",
    "You'll notice that I need to write code to remove the keyword that was searched for, from the anlaysis.\n",
    "Next iteration..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "# adjust stop words\n",
    "ind.stop_words = \"experi\"\n",
    "ind.add_stop_words()\n",
    "\n",
    "words_in_radius = ind.find_words_in_radius(corpus_stem, 'experi', 5)\n",
    "mat, fea = ind.vectorizer(words_in_radius, max_features=30, n_min=1)\n",
    "ind.plot_features(fea, mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "# adjust stop words\n",
    "ind.stop_words = \"skill\"\n",
    "ind.add_stop_words()\n",
    "\n",
    "words_in_radius = ind.find_words_in_radius(corpus_stem, 'skill', 5)\n",
    "mat, fea = ind.vectorizer(words_in_radius, max_features=30, n_min=1)\n",
    "ind.plot_features(fea, mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Job Postings Per City"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grp = df.groupby('city')\n",
    "grp['url'].count().sort_values()[-20:].plot('bar', alpha=0.5, figsize=(14,8), grid=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
