{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Command Tools \n",
    "\n",
    "Ipython run in a terminal is about the best way to work with data interactively. Loading a csv into a Pandas dataframe or just a Numpy array is very powerful. However, there are times when  a handful of shell programs can either save the day or just make the day more productive.\n",
    "\n",
    "Primarliy, I've found the program <i>find</i> to be extremely helpful, and combing it with <i>grep / pcregrep</i> can be enough to get a report out. \n",
    "\n",
    "I also find myself loading data into a table on mySQL as the size of the test data grows. This has the advantage of being a little easier to share with coworkers too.\n",
    "\n",
    "Csvkit was brought to my attention by the O'Reilly book, <u>Data Science on the Command Line</u> and I've come to really like it for small scale work. Pandas can also push and pull data into and out of tables. \n",
    "\n",
    "There are times when keeping my preliminary preprocessing steps completely shell driven is convenient especially if used with a makefile. I've been exploring the use of Drake, since it can encorporate native Python and I think that will be the future. Since GNU Make has been around a long time and is typically installed on any machine you're likely to come across, I've been trying to use it more and appreciate it before making the switch to Drake.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programs you'll need to install\n",
    "* csvkit by onryx, install with pip or conda\n",
    "* pcregrep, Debian sudo apt-get install\n",
    "* parallel, in the Debian repo.\n",
    "\n",
    "## Other\n",
    "I do contrast using Python and Pandas. You can skip running it if you don't already have it installed.\n",
    "\n",
    "* mySQL\n",
    "* Pandas\n",
    "\n",
    "## Links\n",
    "* regular expression: http://www.rexegg.com/\n",
    "* freeing memory: http://unix.stackexchange.com/questions/87908/how-do-you-empty-the-buffers-and-cache-on-a-linux-system\n",
    "* csvkit: https://csvkit.readthedocs.org/en/0.9.1/\n",
    "* stackoverflow on \"find\" program, http://stackoverflow.com/questions/1489277/how-to-use-prune-option-of-find-in-sh\n",
    "\n",
    "\n",
    "# How to use this notebook\n",
    "The ipython notebook is great for Python and there's some support for BASH and SQL. However, it's not perfect yet, and you will have to modify some lines.\n",
    "\n",
    "The cells using bash commands, have the bash magic at the top and a shell variable:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%bash\n",
    "root=\"/home/daniel/git/Python2.7/DataScience/command_line_pres_data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That path is specific to my personal computer and will not work for you. You will need to change **every** occurance to the root path you downloaded the data set to.\n",
    "\n",
    "In the future I'll use this BASH kernel for ipython notebooks:\n",
    "https://github.com/takluyver/bash_kernel\n",
    "\n",
    "The above kernel would allow me to export any shell variables once. \n",
    "I also like this SQL magic provided here: \n",
    "https://github.com/catherinedevlin/ipython-sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook doesn't explain the individual commands in great detail. I am assuming that you either know them, or will be researching them on your own as you work through the commands. I have made some effort to make limited notes about the commands you see. Mainly, these notes are for explaining something not easily understood right away although I still assume you have googled or man paged the command."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Man page\n",
    "If you are very new to the command line you might not know how the man pages work.\n",
    "The manual is opened in vi like environment so navigation might be difficult at first.\n",
    "Pressing _h_ while in the man pages will bring up a help file.\n",
    "\n",
    "Here is summary of a few commands for the man page viewer:\n",
    "* q to quit\n",
    "* / to search\n",
    "* n for next forward match\n",
    "* N for match backwards\n",
    "* h  man page navigation help\n",
    "* j scroll up\n",
    "* k scroll down\n",
    "\n",
    "# Things I don't cover here\n",
    "Quoting is tricky when dealing with the shell. The shell has to interpret ( or not ) everything passed in the command line. That means if there's spaces, or special tokens in the strings you are passing around, you'll have to escape the strings and wrap them in quotes, double or single.\n",
    "\n",
    "This is a topic in itself. I didn't want to get sidetracked here so please read about it. In fact you'll have to\n",
    "because you'll end up banging your head against a wall sooner than later because of it.\n",
    "\n",
    "A nice work around, is to write intermediate steps of a shell script to a file. Then read back the file. Since you are not passing the strings over through the shell interpreter directly through stdin, they will not need special escaping or treatment. There's nothing wrong with this, it can help make things more clear to others, rather than using obscure escape sequences.\n",
    "\n",
    "I made a point to use only _nice_ file names here. Windoze people have a nasty habit of using spaces which is a huge PITA.\n",
    "\n",
    "## In brief:\n",
    "1. Single quotes, ' , are for string literals with no special actions sue to tokens\n",
    "2. Double quotes, \" , allow the expansion of shell variables and parameters, \\$FOO, and \\\\ escape charaters\n",
    "3. back ticks, ` , are completely different and are used to evaluate a command. Although I prefer the \\$(cmd) syntax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Imaginary use case\n",
    "\n",
    "I'm given a hard drive full of source code and told that we will be doing a dependency analysis for some particular C libraries. We don't know in advance what we are looking for at the moment. That may be revealed on a client call or more information will be passed down. For now, we have the directories what can we do with it quickly  ?\n",
    "\n",
    "For this exercise I'm using the Linux Kernel 2.6 because it's sufficiently large and free to download.\n",
    "\n",
    "####The first thing I'd do, is inventory this thing we are given and prepare some basic report about what we have been given."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is is a simple inventory shell script, where we get the lines of code (including blanks, comments everything...not a real good method)\n",
    "and the file extension. This is more advanced than I plan to cover in this notebook. So for now just accept it as is if it's all new to you. \n",
    "\n",
    "###The point is, for this exercise we need an inventory.\n",
    "\n",
    "This code goes into a file named run_inventory.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bash: #: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "%%bash # this line is here for syntax highlighting only\n",
    "\n",
    "#!/bin/bash\n",
    "\n",
    "function count_lines\n",
    "{\n",
    "    input=$1\n",
    "    echo $(cat $input | wc -l)\n",
    "}\n",
    "\n",
    "function check_for_ascii\n",
    "{\n",
    "    input=$1\n",
    "    bool=$(file $input | grep -ic \"ascii\")\n",
    "    if [[ $bool -gt 0 ]];then\n",
    "        echo 1\n",
    "    else\n",
    "        echo 0\n",
    "    fi\n",
    "}\n",
    "\n",
    "function get_extension\n",
    "{\n",
    "    input=\"$1\"\n",
    "    base=$(basename \"$input\")\n",
    "    test_=$(echo \"$base\" | grep -c \"\\.\")\n",
    "\n",
    "    if [ $test_ -eq 0 ];then\n",
    "        echo \"NONE\"\n",
    "    else\n",
    "        ext=$(echo \"$base\" | rev | cut -d. -f 1 | rev)\n",
    "    fi\n",
    "\n",
    "    echo ${ext}\n",
    "}\n",
    "\n",
    "#### calls here ###\n",
    "input=$1\n",
    "\n",
    "count=$(count_lines $input)\n",
    "ascii_bool=$(check_for_ascii $input)\n",
    "ext=$(get_extension $input)\n",
    "\n",
    "if [ $ascii_bool == 1 ];then\n",
    "    printf \"\\\"${input}\\\",\\\"${count}\\\",\\\"${ext}\\\"\\n\"\n",
    "else\n",
    "    printf \"\\\"{$input}\\\",\\\"0\\\",\\\"${ext}\\\"\\n\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Make an inventory\n",
    "The _find_ program is tricky to learn, it has many options. I plan to dive more into _find_, but for now, I really just want an inventory.csv to play with. \n",
    "\n",
    "I give find the path to search, which is our Linux Kernel directory, then I tell find, to only return the type, \"f\" which means files. No directories. \n",
    "\n",
    "I then use a pipe '|' to pass the output into the input of the next program 'run\\_inventory.sh'. We need to use _xargs_ to limit the way the output is presented to 'run\\_inventory.sh'. _Xargs_ is another topic to learn about, so for now, just trust me. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "root=\"/home/daniel/git/Python2.7/DataScience/command_line_pres_data\"\n",
    "#make a header\n",
    "printf \"\\\"path\\\",\\\"nlines\\\",\\\"ext\\\"\\n\" > $root/linux_inventory.csv\n",
    "find $root/linux-2.6.32.67 -type f | xargs -n 1 $root/make_inventory.sh >> $root/linux_inventory.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The > is for stream redirection. The output stream from _printf_ is sent to a new file. This will clobber existing files so practive with it. The second use is slightly different, >> is an appending stream redirection.\n",
    "\n",
    "Since we don't want to clobber the linux_inventory.csv, we use the append operation for the actual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"path\",\"nlines\",\"ext\"\n",
      "\"/home/dcuneo/git/PersonalDS/DataScience/command_line_pres_data/linux-2.6.32.67/.gitignore\",\"77\",\"gitignore\"\n",
      "\"/home/dcuneo/git/PersonalDS/DataScience/command_line_pres_data/linux-2.6.32.67/scripts/.gitignore\",\"10\",\"gitignore\"\n",
      "\"/home/dcuneo/git/PersonalDS/DataScience/command_line_pres_data/linux-2.6.32.67/scripts/module-common.lds\",\"8\",\"lds\"\n",
      "\"/home/dcuneo/git/PersonalDS/DataScience/command_line_pres_data/linux-2.6.32.67/scripts/checkkconfigsymbols.sh\",\"59\",\"sh\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cat: write error: Broken pipe\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "root_dir=\"/home/daniel/git/Python2.7/DataScience/command_line_pres_data\"\n",
    "\n",
    "cat \"${root_dir}/linux_inventory.csv\" | head -n 5 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipe, |, takes output from another process, and supplies it as input to the next program.\n",
    "You will get different output for the two lines below. The first line tells _grep_ to operate on the output stream of the _find_ command. The second tells _grep_ to operate on files whose path is given as the output from _find_. \n",
    "\n",
    "If you don't know what or how _head_ works, try the man page for it. Look up the option _n_ ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "root=\"/home/daniel/git/Python2.7/DataScience/command_line_pres_data\"\n",
    "\n",
    "find $root/linux-2.6.32.67 -type f | grep \"2.6\"\n",
    "#\n",
    "find $root/linux-2.6.32.67 -type f | xargs grep \"2.6\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# let's make a summary of the kinds of files we were given by extension.\n",
    "\n",
    "The next three cells are illustrating the how one can use <i>cat, cut, sort and uniq</i> to get a preliminary output of this inventory.\n",
    "\n",
    "I broke out the steps so that you can follow along. There maybe short cuts and other arguments I didn't use. Sometimes it doesn't matter if you do something the correct way, just that you get it done quickly and you are confident that you know what you did. This type of prototyping is great, because you can see what's happening."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The program <i> cut </i> is super helpful. We can parse many output streams with <i>cut</i>.\n",
    "If the format of a steam in tablular, then <i>awk</i> maybe the best, but <i>awk</i> is a whole animal into itself and I think most days, I rely on <i>cut</i> and then step into ipython use use Pandas or just Numpy.\n",
    "\n",
    "The <i>-d,</i> option is to set the delimiter and the <i>-f 3</i> says to use the 3rd field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"c\"\n",
      "\"c\"\n",
      "\"c\"\n",
      "\"c\"\n",
      "\"NONE\"\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "root_dir=\"/home/daniel/git/Python2.7/DataScience/command_line_pres_data\"\n",
    "cat $root_dir/linux_inventory.csv | cut -d, -f 3 | tail -n 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"y\"\n",
      "\"y\"\n",
      "\"y\"\n",
      "\"y\"\n",
      "\"ymfsb\"\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "root_dir=\"/home/daniel/git/Python2.7/DataScience/command_line_pres_data\"\n",
    "cat \"${root_dir}/linux_inventory.csv\" | cut -d, -f 3 | sort  | tail -n  5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      1 \"x86\"\n",
      "    105 \"xml\"\n",
      "      6 \"xsl\"\n",
      "      5 \"y\"\n",
      "      1 \"ymfsb\"\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "root_dir=\"/home/daniel/git/Python2.7/DataScience/command_line_pres_data\"\n",
    "cat \"${root_dir}/linux_inventory.csv\" | cut -d, -f 3 | sort | uniq -c | tail -n 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    849 \"txt\"\n",
      "   1080 \"S\"\n",
      "   2391 \"NONE\"\n",
      "  11622 \"h\"\n",
      "  13147 \"c\"\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "root_dir=\"/home/daniel/git/Python2.7/DataScience/command_line_pres_data\"\n",
    "cat $root_dir/linux_inventory.csv | cut -d, -f 3 | sort | uniq -ic | sort -n | tail -n 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting this command line data into a database.\n",
    "* Pandas:  df.to_sql()\n",
    "* csvkit: csvsql\n",
    "\n",
    "### Both Pandas and csvkit use SqlAlchemy to handle the connection to the DB. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "root=\"/home/daniel/git/Python2.7/DataScience/command_line_pres_data\"\n",
    "# I'm not expert at SqlAlchemy, but learning the connection strings are quite important\n",
    "# That's the, \"mysql://<log-in>:<passwd>@<ip>/<db-name>\"  I'm running mySQL on locally, localhost has IP 127.0.0.1\n",
    "csvsql --db \"mysql://root:test@127.0.0.1/LinuxKernel\" --table \"inventory\" --insert \"${root}/linux_inventory.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check that it worked\n",
    "\n",
    "The notebook isn't using the alias the way I expected so I had to type out the full command with user and password.\n",
    "In normal practice you'd make an alias as in the comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path\tisText\text\ttype\tsize\tsloc\tcomments\tblank\ttot_lines\n",
      "linux-2.6.32.67/arch/h8300/mm/fault.c\t1\tc\tC source ASCII text\t1441\t23\t28\t6\t57\n",
      "linux-2.6.32.67/arch/h8300/mm/kmap.c\t1\tc\tC source ASCII text\t1276\t27\t25\t8\t60\n",
      "linux-2.6.32.67/arch/h8300/mm/Makefile\t1\tNULL\tASCII text\t114\t0\t0\t0\t5\n",
      "linux-2.6.32.67/arch/h8300/mm/init.c\t1\tc\tC source ASCII text\t5463\t120\t56\t27\t201\n",
      "linux-2.6.32.67/arch/h8300/mm/memory.c\t1\tc\tC source ASCII text\t1099\t26\t21\t9\t56\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "#alias mysql='mysql --user=root --password=test'\n",
    "#mysql -e \"SELECT * FROM inventory LIMIT 5;\" LinuxKernel\n",
    "\n",
    "mysql --user=root --password=test -e \"SELECT * FROM inventory LIMIT 5;\" LinuxKernel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dts,115\n",
      "txt,849\n",
      "S,1080\n",
      "h,11623\n",
      "c,13147\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "sql2csv --db mysql://root:test@127.0.0.1/LinuxKernel --query \"select ext, count(ext) from inventory group by ext order by count(ext);\" | tail -n 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The numbers checkout.\n",
    "\n",
    "#### Lets explore <i>awk</i> a little. The easiest thing to make do (and perhaps the most useful), is to print columns or rows from a file stream.\n",
    "\n",
    "<i>awk</i>\n",
    "* FS field separator set to comma\n",
    "* $3 is the 3rd column\n",
    "\n",
    "<i>grep</i>\n",
    "* -E regular expression\n",
    "* -c count occurance (grep operates per line, we use pcregrep for multiple line searches)\n",
    "\n",
    "There is a way to do the whole regex and count in <i>awk</i> I just didn't want to get into it. I'm still learning <i>awk</i> myself, and I haven't decided on it's usefulness over other tools. If I'm already in a database then forget it...I mostly want to show use of <i>grep</i> and <i>awk</i> for column parsing ( rather than cut )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1699\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "root_dir=\"/home/daniel/git/Python2.7/DataScience/command_line_pres_data\"\n",
    "\n",
    "cat \"${root_dir}/linux_inventory.csv\"  | awk  'FS=\",\"; {print $3}' | grep -cE \"txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ext,count(ext)\n",
      "txt,2491\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "root_dir=\"/home/daniel/git/Python2.7/DataScience/command_line_data\"\n",
    "\n",
    "sql2csv --db mysql://root:test@127.0.0.1/LinuxKernel --query \"select ext, count(ext) from inventory where ext='txt';\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Break down the commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# do not run\n",
    "find \"${root_dir}/linux-2.6.32.67\" -maxdepth 2 -mindepth 1 -type f -iname \"*.c\" | \\\n",
    "    xargs -n 1 pcregrep -no \"(?sim)[a-z]+\\w*\\(.*?\\)\" /dev/null | \\\n",
    "    sed 's/\\s//g' | \\\n",
    "    tail -n 10 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## find ${root_dir}/linux-2.6.32.67\" -maxdepth 2 -mindepth 1 -type f -iname \"*.c\"\n",
    "\n",
    "* path to search\n",
    "* -maxdepth 2 dont', search past 2 directories deep\n",
    "* -mindepth 1, search at least 1 directory deep   (using this to save time and the notebook kept crashing)\n",
    "* -type f, only look for files not directories or links or anything else\n",
    "* -iname,  case insensitive glob style name matching, just like <i>ls</i> uses\n",
    "\n",
    "## xargs\n",
    "A neat helper program, that takes the output from another shell program, and parses it into discrete intput arguments for the next program in a pipe.\n",
    "\n",
    "This will allow us to pass one line at a time from find, to grep. Some programs do not need xargs, as they are designed to take a stream of input. Not in this case however.\n",
    "\n",
    "## pcregrep -no \"(?sim)[a-z]+\\w*\\(.*?\\)\" /dev/null\n",
    "We also introduct, <i>Pearl Compatiple Regular Expression Grep (pcregrep)</i>. \n",
    "The <i>(?sim)</i> are options:\n",
    "\n",
    "* s dot matches everything including newlines \"\\n\"\n",
    "* i case insensitive\n",
    "* m multiline\n",
    "\n",
    "The <i>(.*?)</i> makes the greedy \".\" stop, after encountering a left parenthesis, escaped like this \\\\( \n",
    "This page explains the \"Lazy Trap\" issue, where the .*? contron on the greedy match \"jumps the fence\"\n",
    "http://www.rexegg.com/regex-quantifiers.html#lazytrap\n",
    "\n",
    "### This is going to hunt for functions\n",
    "That is, strings that match (?sim)[a-z]+\\w*\\(.*?\\), where:\n",
    "\n",
    "* [a-z] a list of lower case letters\n",
    "* '+' at least once, or more\n",
    "* \\w any alpha-numeric zero or more times\n",
    "* \\\\( literal left parenthesis\n",
    "* greedy match until right paren\n",
    "\n",
    "The <i>/dev/null</i> is a trick to make grep print the file path it's working on.\n",
    "\n",
    "## sed \n",
    "used to remove any spaces:\n",
    "sed 's/\\s//g'\n",
    "\n",
    "* s  substitute\n",
    "* \\s  regular expression for any kind of while space\n",
    "* //  replace with nothing....easier to read if it was, sed 's/foo/bar/g' , replace 'foo' with 'bar'\n",
    "* g globally, as many times as a match can be made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/daniel/git/Python2.7/DataScience/command_line_pres_data/linux-2.6.32.67/fs/dcookies.c:332:mutex_lock(&dcookie_mutex)\n",
      "/home/daniel/git/Python2.7/DataScience/command_line_pres_data/linux-2.6.32.67/fs/dcookies.c:334:list_del(&user->next)\n",
      "/home/daniel/git/Python2.7/DataScience/command_line_pres_data/linux-2.6.32.67/fs/dcookies.c:335:kfree(user)\n",
      "/home/daniel/git/Python2.7/DataScience/command_line_pres_data/linux-2.6.32.67/fs/dcookies.c:337:is_live()\n",
      "/home/daniel/git/Python2.7/DataScience/command_line_pres_data/linux-2.6.32.67/fs/dcookies.c:338:dcookie_exit()\n",
      "/home/daniel/git/Python2.7/DataScience/command_line_pres_data/linux-2.6.32.67/fs/dcookies.c:340:mutex_unlock(&dcookie_mutex)\n",
      "/home/daniel/git/Python2.7/DataScience/command_line_pres_data/linux-2.6.32.67/fs/dcookies.c:343:EXPORT_SYMBOL_GPL(dcookie_register)\n",
      "/home/daniel/git/Python2.7/DataScience/command_line_pres_data/linux-2.6.32.67/fs/dcookies.c:344:EXPORT_SYMBOL_GPL(dcookie_unregister)\n",
      "/home/daniel/git/Python2.7/DataScience/command_line_pres_data/linux-2.6.32.67/fs/dcookies.c:345:EXPORT_SYMBOL_GPL(get_dcookie)\n",
      "/home/daniel/git/Python2.7/DataScience/command_line_pres_data/linux-2.6.32.67/fs/no-block.c:15:no_blkdev_open(structinode*inode,structfile*filp)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "root_dir=\"/home/daniel/git/Python2.7/DataScience/command_line_pres_data\"\n",
    "\n",
    "find \"${root_dir}/linux-2.6.32.67\" -maxdepth 2 -mindepth 1 -type f -iname \"*.c\" | xargs -n 1 pcregrep -no \"(?sim)[a-z]+\\w*\\(.*?\\)\" /dev/null | sed 's/\\s//g' | tail -n 10 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This example used the regular expression syntax where as above, I used the \"globbing\" rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/daniel/git/Python2.7/DataScience/command_line_pres_data/linux-2.6.32.67/fs/dcookies.c:332:mutex_lock(&dcookie_mutex)\n",
      "/home/daniel/git/Python2.7/DataScience/command_line_pres_data/linux-2.6.32.67/fs/dcookies.c:334:list_del(&user->next)\n",
      "/home/daniel/git/Python2.7/DataScience/command_line_pres_data/linux-2.6.32.67/fs/dcookies.c:335:kfree(user)\n",
      "/home/daniel/git/Python2.7/DataScience/command_line_pres_data/linux-2.6.32.67/fs/dcookies.c:337:is_live()\n",
      "/home/daniel/git/Python2.7/DataScience/command_line_pres_data/linux-2.6.32.67/fs/dcookies.c:338:dcookie_exit()\n",
      "/home/daniel/git/Python2.7/DataScience/command_line_pres_data/linux-2.6.32.67/fs/dcookies.c:340:mutex_unlock(&dcookie_mutex)\n",
      "/home/daniel/git/Python2.7/DataScience/command_line_pres_data/linux-2.6.32.67/fs/dcookies.c:343:EXPORT_SYMBOL_GPL(dcookie_register)\n",
      "/home/daniel/git/Python2.7/DataScience/command_line_pres_data/linux-2.6.32.67/fs/dcookies.c:344:EXPORT_SYMBOL_GPL(dcookie_unregister)\n",
      "/home/daniel/git/Python2.7/DataScience/command_line_pres_data/linux-2.6.32.67/fs/dcookies.c:345:EXPORT_SYMBOL_GPL(get_dcookie)\n",
      "/home/daniel/git/Python2.7/DataScience/command_line_pres_data/linux-2.6.32.67/fs/no-block.c:15:no_blkdev_open(structinode*inode,structfile*filp)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "root_dir=\"/home/daniel/git/Python2.7/DataScience/command_line_pres_data\"\n",
    "find \"${root_dir}/linux-2.6.32.67\" -maxdepth 2 -mindepth 1 -type f -regextype posix-extended -regex \".*\\.(c|h|cpp)\" | \\\n",
    "    xargs -n 1 pcregrep  -no \"(?sim)[a-z]+\\w*\\(.*?\\)\" /dev/null | sed 's/\\s//g' | tail -n 10 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An aside about loops vs the _parallel_ program\n",
    "## While loops in bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process is interrupted.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "root_dir=\"/home/daniel/git/Python2.7/DataScience/command_line_pres_data\"\n",
    "\n",
    "while read f;do   \n",
    "    file_=$(echo $f | cut -d, -f 1 | sed 's/\\\"//g')\n",
    "    grep -Eo -m 1 \"^#include <linux\"  \"${file_}\" /dev/null\n",
    "done < \"${root_dir}/linux_inventory.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root_dir=\"/home/daniel/git/Python2.7/DataScience/command_line_pres_data\"\n",
      "\n",
      "input_=$1\n",
      "file_=$(echo $input_ | cut -d, -f 1 | sed s/\\\"//g)\n",
      "out=$(grep -Eo -m 1 \"^#include <linux\"  \"${root_dir}/${file_}\" /dev/null)\n",
      "echo $out\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "root_dir=\"/home/daniel/git/Python2.7/DataScience/command_line_pres_data\"\n",
    "\n",
    "cat $root_dir/prep.sh "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/daniel/git/Python2.7/DataScience/command_line_pres_data/linux-2.6.32.67/drivers/rapidio/rio.c:#include <linux\n",
      "/home/daniel/git/Python2.7/DataScience/command_line_pres_data/linux-2.6.32.67/drivers/rapidio/switches/tsi500.c:#include <linux\n",
      "/home/daniel/git/Python2.7/DataScience/command_line_pres_data/linux-2.6.32.67/drivers/rapidio/rio.h:#include <linux\n",
      "\n",
      "/home/daniel/git/Python2.7/DataScience/command_line_pres_data/linux-2.6.32.67/drivers/rapidio/rio-access.c:#include <linux\n",
      "/home/daniel/git/Python2.7/DataScience/command_line_pres_data/linux-2.6.32.67/drivers/rapidio/rio-scan.c:#include <linux\n",
      "\n",
      "/home/daniel/git/Python2.7/DataScience/command_line_pres_data/linux-2.6.32.67/drivers/rapidio/rio-sysfs.c:#include <linux\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "root_dir=\"/home/daniel/git/Python2.7/DataScience/command_line_pres_data\"\n",
    "\n",
    "tail -n 10 \"${root_dir}/linux_inventory.csv\" | xargs -P 4 -n 1 $root_dir/prep.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <i>Parallel</i>\n",
    "\n",
    "A powerful program that makes running shell programs on multiple cores trivial, is <i>parallel</i> .\n",
    "It also cleans up code and makes things a single line when used with just a single core."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "real\t0m2.031s\n",
      "user\t0m0.828s\n",
      "sys\t0m1.250s\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Free the cache to really test the timing\n",
    "# become root and run \n",
    "# free && sync && echo 3 > /proc/sys/vm/drop_caches && free\n",
    "\n",
    "root_dir=\"/home/daniel/git/Python2.7/DataScience/command_line_pres_data\"\n",
    "time find \"${root_dir}/linux-2.6.32.67\" -maxdepth 2 -mindepth 1 -type f -name \"*.c\" | \\\n",
    "    parallel --jobs 1 -n 1 'pcregrep -no \"(?sm)if\\s*\\(.*?\\)\" /dev/null' | sed 's/\\s//g' > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "find: `/linux-2.6.32.67': No such file or directory\n",
      "\n",
      "real\t0m0.121s\n",
      "user\t0m0.056s\n",
      "sys\t0m0.043s\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# become root and run \n",
    "# free && sync && echo 3 > /proc/sys/vm/drop_caches && free\n",
    "time find \"${root_dir}/linux-2.6.32.67\" -maxdepth 2 -mindepth 1 -type f -name \"*.c\" | \\\n",
    "    parallel --jobs 4 -n 1 'pcregrep -no \"(?sm)if\\s*\\(.*?\\)\" /dev/null' | sed 's/\\s//g' > /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add columns to our inventory database\n",
    "Let's say that we want to add a categorical columns to our SQL table. This is a nice way to store information about a table without wororying about normalization. We are not DBA's trying to maintain strict schema here. We just need a fast and intuitive way to query our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Categorical Variables\n",
    "I had some difficulty when I first started reading about categorical variables. They are writen and talked about in various contexts. For now, I'm thinking about a column in a database like table, that contains a string, which describes the row.\n",
    "\n",
    "In contrast, I could have had a boolean flag like columns. For every file with an include statement that has \"linux\" in it, give it a 1 and the rest a 0. Then for another condition, I'd have to add a flag for it, \"foo\" with 1 and 0's and so on. \n",
    "\n",
    "In this case, a categorical variable is a single column that makes it wasy to do \"group by\", and aggregrate across categories. We can use more logic, such as \"where\" clauses, to define even more granular categories."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "So let's add a column for the presense of the include statement, \"#include <linux/*>\" ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "root_dir=\"/home/daniel/git/Python2.7/DataScience/command_line_pres_data\"\n",
    "\n",
    "find \"${root_dir}/linux-2.6.32.67\" -regextype posix-extended -regex \".*\\.(c|h)\" -type f | \\\n",
    "    parallel --jobs 4 -n 1 'grep -Eo -m 1 \"^#include\\s\\Wlinux/\\w+\\.h\\W\" /dev/null' > \"${root_dir}/sub_list.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "root_dir=\"/home/daniel/git/Python2.7/DataScience/command_line_pres_data\"\n",
    "\n",
    "csvsql --db \"mysql://root:test@127.0.0.1/LinuxKernel\" --table \"inventory\" --insert \"${root_dir}/linux_inventory.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use sed ( stream editor ) to add the header row. You can certainly open the file in an editor and do this manually. I like to practice sed as much as possible b/c it takes a while to learn . I often use this synatx, \n",
    "<i>sed 's/foo/bar/g' to substitue.</i> \n",
    "\n",
    "However, this a an \"address\" command and to be honest I've been forgetting how to use it, but that's what this Meetup is all about, practice through teaching and sharing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "root_dir=\"/home/daniel/git/Python2.7/DataScience/command_line_pres_data\"\n",
    "\n",
    "sed -i '1 a\\path_:include' \"${root_dir}/sub_list.txt\" # address command, 'append' at first line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "root_dir=\"/home/daniel/git/Python2.7/DataScience/command_line_pres_data\"\n",
    "\n",
    "csvsql -d \":\" --db \"mysql://root:test@127.0.0.1/LinuxKernel\" --table \"sub_list\" --insert \"${root_dir}/sub_list.txt\"  "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "update inventory set category = \n",
    "       case when inventory.path in \n",
    "\t     (select path_ from sub_list)\n",
    "       then \"linux\" else NULL\n",
    " end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sql=\"update inventory set category = case when inventory.path in (select path_ from sub_list) then 'linux' else NULL end;\"\n",
    "\n",
    "mysql -e \"${sql}\" LinuxKernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "sql2csv --db \"mysql://root:test@127.0.0.1/LinuxKernel\" --table \"inventory\" --query \"SELECT * from inventory;\" > linux_inventory_include.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "root_dir=\"/home/daniel/git/Python2.7/DataScience/command_line_pres_data\"\n",
    "\n",
    "\n",
    "find \"${root_dir}/linux-2.6.32.67\" -regextype posix-extended -regex \".*\\.(c|h)\" -type f | \\\n",
    "    parallel --jobs 4 -n 1 'grep -Eo -m 1 \"^#include\\s\\Wlinux/kernel\\.h\\W\" /dev/null' > linux_inventory_kernel.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do the same thing with Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sqlalchemy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import path\n",
    "root_dir = \"/home/daniel/git/Python2.7/DataScience/command_line_pres_data\"\n",
    "engine = sqlalchemy.create_engine(\"mysql://root:test@127.0.0.1/LinuxKernel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What if we did the find | grep steps in  Python....? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "ob = re.compile(\"#include\\s\\Wlinux/(?P<name>\\w+)\\.h\\W\")\n",
    "\n",
    "def regex_check(filename):\n",
    "    fname = path.join(root_dir, filename)\n",
    "    f = open(fname, 'r')\n",
    "    match = ob.search(f.read())\n",
    "    f.close()\n",
    "    \n",
    "    if match:\n",
    "        return match.group('name')\n",
    "    \n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rio\n"
     ]
    }
   ],
   "source": [
    "print regex_check('linux-2.6.32.67/drivers/rapidio/rio-access.c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "src = path.join(root_dir, \"linux_inventory.csv\")\n",
    "df = pd.read_csv(src, quotechar='\"', quoting=1)\n",
    "df['category'] = df.apply(lambda row: regex_check(row['path']) if row['ext'] == 'h' or row['ext'] == 'c' else None, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          mutex\n",
       "1             if\n",
       "2         kernel\n",
       "3    etherdevice\n",
       "4            err\n",
       "5         bitops\n",
       "6           None\n",
       "7         kernel\n",
       "8         device\n",
       "9           None\n",
       "Name: category, dtype: object"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['category'][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.to_sql(\"inventory2\", engine, flavor='mysql')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cts\tcategory\n",
      "15235\tNULL\n",
      "3002\tmodule\n",
      "2266\tkernel\n",
      "1534\ttypes\n",
      "1327\tinit\n",
      "319\tdelay\n",
      "318\terrno\n",
      "259\tfs\n",
      "245\tsched\n",
      "241\tmm\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "sql=\"select count(path) as cts, category from inventory2 group by category order by cts desc limit 10;\"\n",
    "mysql -uroot -ptest -e \"${sql}\" LinuxKernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "root_dir=\"/home/daniel/git/Python2.7/DataScience/command_line_pres_data\"\n",
    "\n",
    "sql2csv --db \"mysql://root:test@127.0.0.1/LinuxKernel\" --query \"select count(path) as cts, category from inventory2 group by category order by cts desc;\" \\\n",
    "> \"${root_dir}/categories.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back to Command line\n",
    "\n",
    "## Save the output to a file for the future\n",
    "We'll redirect the output from standard out (terminal display) to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "#root_dir=\"/home/daniel/git/Python2.7/DataScience/command_line_data\"\n",
    "root_dir=\"/home/dcuneo/git/PersonalDS/DataScience/command_line_pres_data\"\n",
    "cat $root_dir/linux_inventory.csv | cut -d, -f 3 | sort | uniq -c | sort -n > $root_dir/ext_list.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Makefile\n",
    "Let's try using a makefile. We have 2 steps required to create the sorted list of extensions and their counts.\n",
    "\n",
    "1. make an inventory\n",
    "2. sort the inventory by extension\n",
    "\n",
    "We also have 2 dependencies\n",
    "\n",
    "1. Linux kernel source\n",
    "2. inventory\n",
    "\n",
    "There's one final output, the sorted list of counts by extension\n",
    "\n",
    "The idea behind the makefile, is that if we change a dependency, then we want the target output steps to run again.\n",
    "If a file was added to the Linux kernel, then we need a new inventory and then a file extension count list. in the a real world usage I'd make some more effort to avoid running the entire inventory. Here, that's a bit overkill.\n",
    "\n",
    "\n",
    "## What is happening\n",
    "Make keeps track of when a file or directory has been modified. If something was touched, then the recipe is invoked to handle that updated information. Make can make use of functions, shell paramters although it a slightly different form.\n",
    "\n",
    "Makefile are typically named, \"makefile\", and are tab delimited. Make has to parse the makefile so there's some special syntax that is similar to but distinct from that of the shell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cat: makefile: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "#root_dir=\"/home/daniel/git/Python2.7/DataScience/command_line_data\"\n",
    "root_dir=\"/home/dcuneo/git/PersonalDS/DataScience/command_line_pres_data\"\n",
    "cat -n makefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     1\t# simple makefile for creating a new inventory and extension list\n",
      "     2\t# if the kernel is updated with new source\n",
      "     3\t\n",
      "     4\t# global path prefix\n",
      "     5\troot_dir = /home/dcuneo/git/PersonalDS/DataScience/command_line_pres_data\n",
      "     6\t\n",
      "     7\t# prerequistes\n",
      "     8\tkernel = $(root_dir)/linux-2.6.32.67\n",
      "     9\tinventory = $(root_dir)/linux_inventory.csv\n",
      "    10\t\n",
      "    11\t# target\n",
      "    12\textension_list = $(root_dir)/ext_list.txt\n",
      "    13\t\n",
      "    14\t# shell script required for recipe\n",
      "    15\tmake_inventory = $(root_dir)/make_inventory.sh\n",
      "    16\t\n",
      "    17\t\n",
      "    18\t$(extension_list): $(inventory)\n",
      "    19\t\tcat $(inventory) | cut -d, -f 3 | sort | uniq -c | sort -n > $(extension_list)\n",
      "    20\t\n",
      "    21\t\n",
      "    22\t$(inventory): $(kernel)\n",
      "    23\t\tfind $(kernel) -type f | parallel -n 1 --jobs 2 $(make_inventory) > $(inventory)\n",
      "    24\t\tsed -i '1 i\\path_,tot_lines' $(inventory)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "#root_dir=\"/home/daniel/git/Python2.7/DataScience/command_line_data\"\n",
    "root_dir=\"/home/dcuneo/git/PersonalDS/DataScience/command_line_pres_data\"\n",
    "cat -n $root_dir/makefile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test it out\n",
    "\n",
    "We can use the _touch_ command to update the modifcation dates of a file or directory.\n",
    "There are at least 3 ways to see the modifcation dates of a file, the most common being to _stat_. incidentaly, _stat_ is a really basic program that is called internally in many other shell programs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  File: ‘/home/dcuneo/git/PersonalDS/DataScience/command_line_pres_data/linux-2.6.32.67’\n",
      "  Size: 4096      \tBlocks: 8          IO Block: 4096   directory\n",
      "Device: 805h/2053d\tInode: 7494692     Links: 23\n",
      "Access: (0775/drwxrwxr-x)  Uid: ( 1000/  dcuneo)   Gid: ( 1000/  dcuneo)\n",
      "Access: 2015-09-29 18:31:19.851654480 -0700\n",
      "Modify: 2015-09-29 18:31:19.795654477 -0700\n",
      "Change: 2015-09-29 18:31:19.795654477 -0700\n",
      " Birth: -\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "#root_dir=\"/home/daniel/git/Python2.7/DataScience/command_line_data\"\n",
    "root_dir=\"/home/dcuneo/git/PersonalDS/DataScience/command_line_pres_data\"\n",
    "stat $root_dir/linux-2.6.32.67"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The program _touch_ is used to update the modification time to the present date. It is also used to create an empty file when one needs such a thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  File: ‘/home/dcuneo/git/PersonalDS/DataScience/command_line_pres_data/linux-2.6.32.67’\n",
      "  Size: 4096      \tBlocks: 8          IO Block: 4096   directory\n",
      "Device: 805h/2053d\tInode: 7494692     Links: 23\n",
      "Access: (0775/drwxrwxr-x)  Uid: ( 1000/  dcuneo)   Gid: ( 1000/  dcuneo)\n",
      "Access: 2015-09-29 18:51:46.139712980 -0700\n",
      "Modify: 2015-09-29 18:51:46.139712980 -0700\n",
      "Change: 2015-09-29 18:51:46.139712980 -0700\n",
      " Birth: -\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "#root_dir=\"/home/daniel/git/Python2.7/DataScience/command_line_data\"\n",
    "root_dir=\"/home/dcuneo/git/PersonalDS/DataScience/command_line_pres_data\"\n",
    "touch $root_dir/linux-2.6.32.67\n",
    "stat $root_dir/linux-2.6.32.67"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "find /home/dcuneo/git/PersonalDS/DataScience/command_line_pres_data/linux-2.6.32.67 -type f | parallel -n 1 --jobs 2 /home/dcuneo/git/PersonalDS/DataScience/command_line_pres_data/make_inventory.sh > /home/dcuneo/git/PersonalDS/DataScience/command_line_pres_data/linux_inventory.csv\n",
      "sed -i '1 i\\path_,tot_lines' /home/dcuneo/git/PersonalDS/DataScience/command_line_pres_data/linux_inventory.csv\n",
      "cat /home/dcuneo/git/PersonalDS/DataScience/command_line_pres_data/linux_inventory.csv | cut -d, -f 3 | sort | uniq -c | sort -n > /home/dcuneo/git/PersonalDS/DataScience/command_line_pres_data/ext_list.txt\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "#root_dir=\"/home/daniel/git/Python2.7/DataScience/command_line_data\"\n",
    "root_dir=\"/home/dcuneo/git/PersonalDS/DataScience/command_line_pres_data\"\n",
    "# if we are in the directory where the makefile exists, just issue the _make_ command\n",
    "# because this notebook could theoretically be run from anywhere, I'm using the full path and the -f option\n",
    "make -f $root_dir/makefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  File: ‘/home/dcuneo/git/PersonalDS/DataScience/command_line_pres_data/ext_list.txt’\n",
      "  Size: 3435      \tBlocks: 8          IO Block: 4096   regular file\n",
      "Device: 805h/2053d\tInode: 7354139     Links: 1\n",
      "Access: (0664/-rw-rw-r--)  Uid: ( 1000/  dcuneo)   Gid: ( 1000/  dcuneo)\n",
      "Access: 2015-09-29 19:01:56.123742080 -0700\n",
      "Modify: 2015-09-29 18:58:19.531731747 -0700\n",
      "Change: 2015-09-29 18:58:19.531731747 -0700\n",
      " Birth: -\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "#root_dir=\"/home/daniel/git/Python2.7/DataScience/command_line_data\"\n",
    "root_dir=\"/home/dcuneo/git/PersonalDS/DataScience/command_line_pres_data\"\n",
    "\n",
    "stat $root_dir/ext_list.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing the output of command to a variable (shell paramter)\n",
    "\n",
    "We can set a shell paramter from the output of another shell command/program.\n",
    "You've seen this trick used in the simple inventory program earlier. I used it a lot actually, to \n",
    "assign the output of commands to a shell paramter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dcuneo\n",
      "dcun\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "echo $USER # shell variable setup when you login\n",
    "var=$(echo $USER | cut -c 1-4)\n",
    "echo $var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another example with the _date_ program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Sep 28 19:38:23 PDT 2015\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/dcuneo/git/PersonalDS/DataScience/command_line_pres_data/2015_09_28_19:39:13\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "root_dir=\"/home/dcuneo/git/PersonalDS/DataScience/command_line_pres_data\"\n",
    "\n",
    "dir_path=${root_dir}/$(date +%Y_%m_%d_%H:%M:%S) # date can take a format string\n",
    "echo $dir_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shell Loops\n",
    "\n",
    "There are two kinds of loops that I tend to use:\n",
    "\n",
    "1. while%%bash\n",
    "2. for\n",
    "\n",
    "### Tests\n",
    "\n",
    "The square brackets are called \"tests\" . This is another topic in shell scripting that I can't really cover right here but you can see a use case for it. \n",
    "\n",
    "The loop below is rather contrived. We would really just use _cat_ command to see the contents. But it's a good practice because the output is predictable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 \"1992-1997\"\n",
      "1 \"1994-2004\"\n",
      "1 \"1995-2002\"\n",
      "1 \"1996-2002\"\n",
      "1 \"278\"\n",
      "1 \"5\"\n",
      "1 \"act2000\"\n",
      "1 \"AddingFirmware\"\n",
      "1 \"AdvancedTopics\"\n",
      "1 \"agh\"\n",
      "1 \"aic79xx\"\n",
      "1 \"aic7xxx\"\n",
      "1 \"arcmsr\"\n",
      "1 \"asp\"\n",
      "1 \"au0828\"\n",
      "1 \"audio\"\n",
      "1 \"auto\"\n",
      "1 \"avmb1\"\n",
      "1 \"awk\"\n",
      "1 \"ax\"\n",
      "1 \"binfmt\"\n",
      "1 \"bttv\"\n",
      "1 \"buddha\"\n",
      "1 \"build\"\n",
      "1 \"CAPI\"\n",
      "1 \"cc\"\n",
      "1 \"cert\"\n",
      "1 \"ChangeLog\"\n",
      "1 \"char\"\n",
      "1 \"clean\"\n",
      "1 \"Coding\"\n",
      "1 \"common\"\n",
      "1 \"concap\"\n",
      "1 \"Conclusion\"\n",
      "1 \"copyright\"\n",
      "1 \"cpia\"\n",
      "1 \"cpia2\"\n",
      "1 \"cputype\"\n",
      "1 \"cx23885\"\n",
      "1 \"cycladesZ\"\n",
      "1 \"DAC960\"\n",
      "1 \"dino\"\n",
      "1 \"diversion\"\n",
      "1 \"DOC\"\n",
      "1 \"drm\"\n",
      "1 \"drv_ba_resend\"\n",
      "1 \"dtc\"\n",
      "1 \"dvb-usb\"\n",
      "1 \"Early-stage\"\n",
      "1 \"em28xx\"\n",
      "1 \"ext\"\n",
      "1 \"FIRST\"\n",
      "1 \"FlashPoint\"\n",
      "1 \"Followthrough\"\n",
      "1 \"FPE\"\n",
      "1 \"freeze\"\n",
      "1 \"freezer\"\n",
      "1 \"fwinst\"\n",
      "1 \"gate\"\n",
      "1 \"gdbinit_200MHz_16MB\"\n",
      "1 \"gdbinit_300MHz_32MB\"\n",
      "1 \"gdbinit_400MHz_32MB\"\n",
      "1 \"generic\"\n",
      "1 \"gigaset\"\n",
      "1 \"glade\"\n",
      "1 \"headersinst\"\n",
      "1 \"hfc-pci\"\n",
      "1 \"HiSax\"\n",
      "1 \"history\"\n",
      "1 \"hm12\"\n",
      "1 \"host\"\n",
      "1 \"hp300\"\n",
      "1 \"hysdn\"\n",
      "1 \"hz\"\n",
      "1 \"i2400m\"\n",
      "1 \"icn\"\n",
      "1 \"ide\"\n",
      "1 \"include\"\n",
      "1 \"inc_shipped\"\n",
      "1 \"inf\"\n",
      "1 \"ini\"\n",
      "1 \"Intro\"\n",
      "1 \"ioctl\"\n",
      "1 \"iosched\"\n",
      "1 \"ips\"\n",
      "1 \"ipw2100\"\n",
      "1 \"ipw2200\"\n",
      "1 \"ir\"\n",
      "1 \"kgdb\"\n",
      "1 \"kmemcheck\"\n",
      "1 \"lib\"\n",
      "1 \"LIB\"\n",
      "1 \"libfdt\"\n",
      "1 \"Locking\"\n",
      "1 \"lpfc\"\n",
      "1 \"mailmap\"\n",
      "1 \"maya44\"\n",
      "1 \"megaraid_sas\"\n",
      "1 \"mISDN\"\n",
      "1 \"mm\"\n",
      "1 \"modes\"\n",
      "1 \"modinst\"\n",
      "1 \"modpost\"\n",
      "1 \"modules\"\n",
      "1 \"ncr53c8xx\"\n",
      "1 \"net\"\n",
      "1 \"netlink\"\n",
      "1 \"nommu_defconfig\"\n",
      "1 \"normal\"\n",
      "1 \"opsp_defconfig\"\n",
      "1 \"OSS\"\n",
      "1 \"pcbit\"\n",
      "1 \"platform\"\n",
      "1 \"png\"\n",
      "1 \"Posting\"\n",
      "1 \"preempt\"\n",
      "1 \"Process\"\n",
      "1 \"pvrusb2\"\n",
      "1 \"qla2xxx\"\n",
      "1 \"qla3xxx\"\n",
      "1 \"qlge\"\n",
      "1 \"quirks\"\n",
      "1 \"README\"\n",
      "1 \"rest\"\n",
      "1 \"rules\"\n",
      "1 \"saa7164\"\n",
      "1 \"sb1000\"\n",
      "1 \"sc\"\n",
      "1 \"sed\"\n",
      "1 \"smp\"\n",
      "1 \"SRC\"\n",
      "1 \"sym53c8xx\"\n",
      "1 \"sym53c8xx_2\"\n",
      "1 \"syncppp\"\n",
      "1 \"tex\"\n",
      "1 \"tuner\"\n",
      "1 \"um\"\n",
      "1 \"uni\"\n",
      "1 \"usbvision\"\n",
      "1 \"vbi\"\n",
      "1 \"vdec2\"\n",
      "1 \"vdec2_defconfig\"\n",
      "1 \"vim\"\n",
      "1 \"WARNING\"\n",
      "1 \"wimax\"\n",
      "1 \"WINVIEW\"\n",
      "1 \"x25\"\n",
      "1 \"x86\"\n",
      "1 \"ymfsb\"\n",
      "2 \"asm\"\n",
      "2 \"cx88\"\n",
      "2 \"dat\"\n",
      "2 \"FAQ\"\n",
      "2 \"fax\"\n",
      "2 \"fig\"\n",
      "2 \"gperf\"\n",
      "2 \"html\"\n",
      "2 \"ids\"\n",
      "2 \"inl\"\n",
      "2 \"ivtv\"\n",
      "2 \"megaraid\"\n",
      "2 \"mk\"\n",
      "2 \"nommu\"\n",
      "2 \"pbm\"\n",
      "2 \"saa7134\"\n",
      "2 \"ucode\"\n",
      "2 \"up_defconfig\"\n",
      "3 \"gdbinit\"\n",
      "3 \"map\"\n",
      "3 \"py\"\n",
      "3 \"reg\"\n",
      "3 \"scr\"\n",
      "3 \"seq\"\n",
      "3 \"smp_defconfig\"\n",
      "4 \"cpu\"\n",
      "4 \"doc\"\n",
      "4 \"in\"\n",
      "4 \"inc\"\n",
      "4 \"ld\"\n",
      "4 \"uc\"\n",
      "5 \"l\"\n",
      "5 \"y\"\n",
      "6 \"conf\"\n",
      "6 \"xsl\"\n",
      "7 \"gif\"\n",
      "7 \"H16\"\n",
      "7 \"sa\"\n",
      "9 \"h_shipped\"\n",
      "11 \"c_shipped\"\n",
      "13 \"tst\"\n",
      "14 \"ppm\"\n",
      "15 \"lds\"\n",
      "23 \"pl\"\n",
      "26 \"HEX\"\n",
      "28 \"debug\"\n",
      "33 \"tmpl\"\n",
      "34 \"sh\"\n",
      "50 \"boot\"\n",
      "79 \"gitignore\"\n",
      "105 \"xml\"\n",
      "111 \"ihex\"\n",
      "115 \"dts\"\n",
      "857 \"txt\"\n",
      "1080 \"S\"\n",
      "2818 \"NONE\"\n",
      "11638 \"h\"\n",
      "13154 \"c\"\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "root_dir=\"/home/dcuneo/git/PersonalDS/DataScience/command_line_pres_data\"\n",
    "\n",
    "while read f;do\n",
    "    echo $f\n",
    "done < $root_dir/ext_list.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bash will delimit the output of the cat, by spaces or newlines (\\n). So in order to get the output\n",
    "as we'd like, we need each line to be delimited by \\n, thus the IFS syntax.\n",
    "\n",
    "Read is a better way to work with the contents of a file where it's assumed that you want data on a per-line basis.\n",
    "Most of the time we do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      1 \"1992-1997\"\n",
      "      1 \"1994-2004\"\n",
      "      1 \"1995-2002\"\n",
      "      1 \"1996-2002\"\n",
      "      1 \"278\"\n",
      "      1 \"5\"\n",
      "      1 \"act2000\"\n",
      "      1 \"AddingFirmware\"\n",
      "      1 \"AdvancedTopics\"\n",
      "      1 \"agh\"\n",
      "      1 \"aic79xx\"\n",
      "      1 \"aic7xxx\"\n",
      "      1 \"arcmsr\"\n",
      "      1 \"asp\"\n",
      "      1 \"au0828\"\n",
      "      1 \"audio\"\n",
      "      1 \"auto\"\n",
      "      1 \"avmb1\"\n",
      "      1 \"awk\"\n",
      "      1 \"ax\"\n",
      "      1 \"binfmt\"\n",
      "      1 \"bttv\"\n",
      "      1 \"buddha\"\n",
      "      1 \"build\"\n",
      "      1 \"CAPI\"\n",
      "      1 \"cc\"\n",
      "      1 \"cert\"\n",
      "      1 \"ChangeLog\"\n",
      "      1 \"char\"\n",
      "      1 \"clean\"\n",
      "      1 \"Coding\"\n",
      "      1 \"common\"\n",
      "      1 \"concap\"\n",
      "      1 \"Conclusion\"\n",
      "      1 \"copyright\"\n",
      "      1 \"cpia\"\n",
      "      1 \"cpia2\"\n",
      "      1 \"cputype\"\n",
      "      1 \"cx23885\"\n",
      "      1 \"cycladesZ\"\n",
      "      1 \"DAC960\"\n",
      "      1 \"dino\"\n",
      "      1 \"diversion\"\n",
      "      1 \"DOC\"\n",
      "      1 \"drm\"\n",
      "      1 \"drv_ba_resend\"\n",
      "      1 \"dtc\"\n",
      "      1 \"dvb-usb\"\n",
      "      1 \"Early-stage\"\n",
      "      1 \"em28xx\"\n",
      "      1 \"ext\"\n",
      "      1 \"FIRST\"\n",
      "      1 \"FlashPoint\"\n",
      "      1 \"Followthrough\"\n",
      "      1 \"FPE\"\n",
      "      1 \"freeze\"\n",
      "      1 \"freezer\"\n",
      "      1 \"fwinst\"\n",
      "      1 \"gate\"\n",
      "      1 \"gdbinit_200MHz_16MB\"\n",
      "      1 \"gdbinit_300MHz_32MB\"\n",
      "      1 \"gdbinit_400MHz_32MB\"\n",
      "      1 \"generic\"\n",
      "      1 \"gigaset\"\n",
      "      1 \"glade\"\n",
      "      1 \"headersinst\"\n",
      "      1 \"hfc-pci\"\n",
      "      1 \"HiSax\"\n",
      "      1 \"history\"\n",
      "      1 \"hm12\"\n",
      "      1 \"host\"\n",
      "      1 \"hp300\"\n",
      "      1 \"hysdn\"\n",
      "      1 \"hz\"\n",
      "      1 \"i2400m\"\n",
      "      1 \"icn\"\n",
      "      1 \"ide\"\n",
      "      1 \"include\"\n",
      "      1 \"inc_shipped\"\n",
      "      1 \"inf\"\n",
      "      1 \"ini\"\n",
      "      1 \"Intro\"\n",
      "      1 \"ioctl\"\n",
      "      1 \"iosched\"\n",
      "      1 \"ips\"\n",
      "      1 \"ipw2100\"\n",
      "      1 \"ipw2200\"\n",
      "      1 \"ir\"\n",
      "      1 \"kgdb\"\n",
      "      1 \"kmemcheck\"\n",
      "      1 \"lib\"\n",
      "      1 \"LIB\"\n",
      "      1 \"libfdt\"\n",
      "      1 \"Locking\"\n",
      "      1 \"lpfc\"\n",
      "      1 \"mailmap\"\n",
      "      1 \"maya44\"\n",
      "      1 \"megaraid_sas\"\n",
      "      1 \"mISDN\"\n",
      "      1 \"mm\"\n",
      "      1 \"modes\"\n",
      "      1 \"modinst\"\n",
      "      1 \"modpost\"\n",
      "      1 \"modules\"\n",
      "      1 \"ncr53c8xx\"\n",
      "      1 \"net\"\n",
      "      1 \"netlink\"\n",
      "      1 \"nommu_defconfig\"\n",
      "      1 \"normal\"\n",
      "      1 \"opsp_defconfig\"\n",
      "      1 \"OSS\"\n",
      "      1 \"pcbit\"\n",
      "      1 \"platform\"\n",
      "      1 \"png\"\n",
      "      1 \"Posting\"\n",
      "      1 \"preempt\"\n",
      "      1 \"Process\"\n",
      "      1 \"pvrusb2\"\n",
      "      1 \"qla2xxx\"\n",
      "      1 \"qla3xxx\"\n",
      "      1 \"qlge\"\n",
      "      1 \"quirks\"\n",
      "      1 \"README\"\n",
      "      1 \"rest\"\n",
      "      1 \"rules\"\n",
      "      1 \"saa7164\"\n",
      "      1 \"sb1000\"\n",
      "      1 \"sc\"\n",
      "      1 \"sed\"\n",
      "      1 \"smp\"\n",
      "      1 \"SRC\"\n",
      "      1 \"sym53c8xx\"\n",
      "      1 \"sym53c8xx_2\"\n",
      "      1 \"syncppp\"\n",
      "      1 \"tex\"\n",
      "      1 \"tuner\"\n",
      "      1 \"um\"\n",
      "      1 \"uni\"\n",
      "      1 \"usbvision\"\n",
      "      1 \"vbi\"\n",
      "      1 \"vdec2\"\n",
      "      1 \"vdec2_defconfig\"\n",
      "      1 \"vim\"\n",
      "      1 \"WARNING\"\n",
      "      1 \"wimax\"\n",
      "      1 \"WINVIEW\"\n",
      "      1 \"x25\"\n",
      "      1 \"x86\"\n",
      "      1 \"ymfsb\"\n",
      "      2 \"asm\"\n",
      "      2 \"cx88\"\n",
      "      2 \"dat\"\n",
      "      2 \"FAQ\"\n",
      "      2 \"fax\"\n",
      "      2 \"fig\"\n",
      "      2 \"gperf\"\n",
      "      2 \"html\"\n",
      "      2 \"ids\"\n",
      "      2 \"inl\"\n",
      "      2 \"ivtv\"\n",
      "      2 \"megaraid\"\n",
      "      2 \"mk\"\n",
      "      2 \"nommu\"\n",
      "      2 \"pbm\"\n",
      "      2 \"saa7134\"\n",
      "      2 \"ucode\"\n",
      "      2 \"up_defconfig\"\n",
      "      3 \"gdbinit\"\n",
      "      3 \"map\"\n",
      "      3 \"py\"\n",
      "      3 \"reg\"\n",
      "      3 \"scr\"\n",
      "      3 \"seq\"\n",
      "      3 \"smp_defconfig\"\n",
      "      4 \"cpu\"\n",
      "      4 \"doc\"\n",
      "      4 \"in\"\n",
      "      4 \"inc\"\n",
      "      4 \"ld\"\n",
      "      4 \"uc\"\n",
      "      5 \"l\"\n",
      "      5 \"y\"\n",
      "      6 \"conf\"\n",
      "      6 \"xsl\"\n",
      "      7 \"gif\"\n",
      "      7 \"H16\"\n",
      "      7 \"sa\"\n",
      "      9 \"h_shipped\"\n",
      "     11 \"c_shipped\"\n",
      "     13 \"tst\"\n",
      "     14 \"ppm\"\n",
      "     15 \"lds\"\n",
      "     23 \"pl\"\n",
      "     26 \"HEX\"\n",
      "     28 \"debug\"\n",
      "     33 \"tmpl\"\n",
      "     34 \"sh\"\n",
      "     50 \"boot\"\n",
      "     79 \"gitignore\"\n",
      "    105 \"xml\"\n",
      "    111 \"ihex\"\n",
      "    115 \"dts\"\n",
      "    857 \"txt\"\n",
      "   1080 \"S\"\n",
      "   2818 \"NONE\"\n",
      "  11638 \"h\"\n",
      "  13154 \"c\"\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "root_dir=\"/home/dcuneo/git/PersonalDS/DataScience/command_line_pres_data\"\n",
    "\n",
    "IFS=$'\\n'\n",
    "for f in $(cat $root_dir/ext_list.txt);do                                                                                 \n",
    "    echo $f\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do something slighly more interesting and introduce the _test_ while were at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 \"c_shipped\"\n",
      "13 \"tst\"\n",
      "14 \"ppm\"\n",
      "15 \"lds\"\n",
      "23 \"pl\"\n",
      "26 \"HEX\"\n",
      "28 \"debug\"\n",
      "33 \"tmpl\"\n",
      "34 \"sh\"\n",
      "50 \"boot\"\n",
      "79 \"gitignore\"\n",
      "105 \"xml\"\n",
      "111 \"ihex\"\n",
      "115 \"dts\"\n",
      "857 \"txt\"\n",
      "1080 \"S\"\n",
      "2818 \"NONE\"\n",
      "11638 \"h\"\n",
      "13154 \"c\"\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "root_dir=\"/home/dcuneo/git/PersonalDS/DataScience/command_line_pres_data\"\n",
    "\n",
    "while read f;do\n",
    "        count=$(echo $f | awk '{print $1}')\n",
    "    if [ $count -gt 10 ];then\n",
    "        echo $f\n",
    "    fi\n",
    "done < $root_dir/ext_list.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use case\n",
    "Maybe you run a program in a loop, and save each output to a new file named with the dat and time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/dcuneo/git/PersonalDS/DataScience/command_line_pres_data/2015_09_28_19:41:38.test\n",
      "/home/dcuneo/git/PersonalDS/DataScience/command_line_pres_data/2015_09_28_19:41:38.test\n",
      "/home/dcuneo/git/PersonalDS/DataScience/command_line_pres_data/2015_09_28_19:41:38.test\n",
      "/home/dcuneo/git/PersonalDS/DataScience/command_line_pres_data/2015_09_28_19:41:38.test\n",
      "/home/dcuneo/git/PersonalDS/DataScience/command_line_pres_data/2015_09_28_19:41:38.test\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "root_dir=\"/home/dcuneo/git/PersonalDS/DataScience/command_line_pres_data\"\n",
    "\n",
    "for ind in {1..5};do \n",
    "    fname=\"${root_dir}/$(date +%Y_%m_%d_%H:%M:%S).test\"\n",
    "    echo $fname\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/dcuneo/git/PersonalDS/DataScience/command_line_pres_data/test_1.txt\n",
      "/home/dcuneo/git/PersonalDS/DataScience/command_line_pres_data/test_2.txt\n",
      "/home/dcuneo/git/PersonalDS/DataScience/command_line_pres_data/test_3.txt\n",
      "/home/dcuneo/git/PersonalDS/DataScience/command_line_pres_data/test_4.txt\n",
      "/home/dcuneo/git/PersonalDS/DataScience/command_line_pres_data/test_5.txt\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "root_dir=\"/home/dcuneo/git/PersonalDS/DataScience/command_line_pres_data\"\n",
    "\n",
    "for ind in {1..5};do \n",
    "    fname=\"${root_dir}/test_$ind.txt\"\n",
    "    echo $fname\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
