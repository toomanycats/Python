{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Command Tools \n",
    "\n",
    "Ipython run in a terminal is about the best way to work with data interactively. Loading a csv into a Pandas dataframe or just a Numpy array is very powerful. However, there are times when  a handful of shell programs can either save the day or just make the day more productive.\n",
    "\n",
    "Primarliy, I've found the program <i>find</i> to be extremely helpful, and combing it with <i>grep / pcregrep</i> can be enough to get a report out. \n",
    "\n",
    "Pandas is super helpful as an interactive tool as well. I also find myself loading data into a table on mySQL or as the size of the test data grows. \n",
    "\n",
    "csvkit was brought to my attention by the O'Reilly book, <u>Data Science on the Command Line</u> and I've come to really like it for small scale work. Pandas also helps as a tools to push and pull data into and out of tables.\n",
    "\n",
    "There are times when keeping my preliminary preprocessing steps completely shell driven is convenient especially if used with a makefile. I've been exploring the use of Drake, since it can encorporate native Python and I think that will be the future ( or a future ). However it's harder to get co-workers to use something so new without some additional conversation, plus installation and setup, etc. Make is on all Linux boxes and  like vi I don't have to explain it or worry about getting it working.\n",
    "\n",
    "#### I made an effort to capitize the SQL commands only for readability. When I'm working and typically working fast, I don't bother.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## useful programs:\n",
    "* csvkit by onryx, install with pip or conda\n",
    "* pcregrep, Debian sudo apt-get install\n",
    "* find, typically already installed the two below are \n",
    "* cut\n",
    "* awk\n",
    "* parallel, in the Debian repo.\n",
    "\n",
    "## Databases\n",
    "* mySQL\n",
    "* mysql-workbench\n",
    "* sqlite3\n",
    "\n",
    "## Links\n",
    "* regular expression: http://www.rexegg.com/\n",
    "* freeing memory: http://unix.stackexchange.com/questions/87908/how-do-you-empty-the-buffers-and-cache-on-a-linux-system\n",
    "* csvkit: https://csvkit.readthedocs.org/en/0.9.1/\n",
    "* stackoverflow on \"find\" program, http://stackoverflow.com/questions/1489277/how-to-use-prune-option-of-find-in-sh\n",
    "\n",
    "\n",
    "# How to use this notebook\n",
    "The ipython notebook is great for Python and there's some support for BASH and SQL. However, it's not perfect yet, and you will have to modify some lines.\n",
    "\n",
    "Many cells that have '%%bash' at the top, also have a shell variable defined like this:\n",
    "\n",
    "root=\"/home/dcuneo/git/PersonalDS/DataScience/command_line_pres_data\"\n",
    "\n",
    "That path is specific to my personal computer and will not work for you. you will need to change **every** occurance to the root path you downloaded the data set to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Imaginary use case\n",
    "\n",
    "I'm given a hard drive full of source code and told that we will be doing a dependency analysis for some particular C libraries. We don't know in advance what we are looking for at the moment. That may be revealed on a client call or more information will be passed down. For now, we have the directories what can we do with it quickly  ?\n",
    "\n",
    "For this exercise I'm using the Linux Kernel 2.6 because it's sufficiently large and free to download.\n",
    "\n",
    "####The first thing I'd do, is inventory this thing we are given and prepare some basic report about what we have been given."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is is a simple inventory shell script, where we get the lines of code ( including blanks, comments everything...not real good method )\n",
    "and the file extension. This is more advanced than I plan to cover in this notebook. So for now just accept it as is if it's all new to you. \n",
    "\n",
    "###The point is, for this exercise we need an inventory.\n",
    "\n",
    "This code goes into a file named run_inventory.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "#!/bin/bash\n",
    "\n",
    "function count_lines\n",
    "{\n",
    "input=$1\n",
    "echo $(cat $input | wc -l)\n",
    "}\n",
    "\n",
    "function check_for_ascii\n",
    "{\n",
    "input=$1\n",
    "bool=$(file $input | grep -ic \"ascii\")\n",
    "if [[ $bool -gt 0 ]];then\n",
    "    echo 1\n",
    "else\n",
    "    echo 0\n",
    "fi\n",
    "}\n",
    "\n",
    "function get_extension\n",
    "{\n",
    "    input=\"$1\"\n",
    "    base=$(basename \"$input\")\n",
    "    test_=$(echo \"$base\" | grep -c \"\\.\")\n",
    "\n",
    "    if [ $test_ -eq 0 ];then\n",
    "        echo \"NONE\"\n",
    "    else\n",
    "        ext=$(echo \"$base\" | rev | cut -d. -f 1 | rev)\n",
    "    fi\n",
    "\n",
    "    echo ${ext}\n",
    "}\n",
    "\n",
    "#### calls here ###\n",
    "input=$1\n",
    "\n",
    "count=$(count_lines $input)\n",
    "ascii_bool=$(check_for_ascii $input)\n",
    "ext=$(get_extension $input)\n",
    "\n",
    "if [ $ascii_bool == 1 ];then\n",
    "    printf \"\\\"${input}\\\",\\\"${count}\\\",\\\"${ext}\\\"\\n\"\n",
    "else\n",
    "    printf \"\\\"{$input}\\\",\\\"0\\\",\\\"${ext}\\\"\\n\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Make an inventory\n",
    "The find program is tricky to learn, it has many options. I plan to dive more into find, but for now, I really just want an inventory.csv to play with. \n",
    "\n",
    "I give find the path to search, which is our Linux Kernel directory, then I tell find, to only return the type, \"f\" which means files. No directories. \n",
    "\n",
    "I then use a pipe '|' to pass the output into the input of the next program 'run_inventory.sh'. We need to use 'xargs' to limit the way the output is presented to 'run_inventory.sh'. Xargs is another topic to learn about, so for now, just trust me. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "root=\"/home/daniel/git/Python2.7/DataScience/command_line_pres_data\"\n",
    "#make a header\n",
    "printf \"\\\"path\\\",\\\"nlines\\\",\\\"ext\\\"\\n\" > $root/linux_inventory.csv\n",
    "find $root/linux-2.6.32.67 -type f | xargs -n 1 $root/make_inventory.sh >> $root/linux_inventory.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"path\",\"nlines\",\"ext\"\n",
      "\"/home/dcuneo/git/PersonalDS/DataScience/command_line_pres_data/linux-2.6.32.67/.gitignore\",\"77\",\"gitignore\"\n",
      "\"/home/dcuneo/git/PersonalDS/DataScience/command_line_pres_data/linux-2.6.32.67/scripts/.gitignore\",\"10\",\"gitignore\"\n",
      "\"/home/dcuneo/git/PersonalDS/DataScience/command_line_pres_data/linux-2.6.32.67/scripts/module-common.lds\",\"8\",\"lds\"\n",
      "\"/home/dcuneo/git/PersonalDS/DataScience/command_line_pres_data/linux-2.6.32.67/scripts/checkkconfigsymbols.sh\",\"59\",\"sh\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cat: write error: Broken pipe\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "root_dir=\"/home/daniel/git/Python2.7/DataScience/command_line_pres_data\"\n",
    "\n",
    "cat \"${root_dir}/linux_inventory.csv\" | head -n 5 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# let's make a summary of the kinds of files we were given by extension.\n",
    "\n",
    "The next three cells are illustrating the how one can use <i>cat, cut, sort and uniq</i> to get a preliminary output of this inventory.\n",
    "\n",
    "I broke out the steps so that you can follow along. There maybe short cuts and other arguments I didn't use. Sometimes it doesn't matter if you do something the correct way, just that you get it done quickly and you are confident that you know what you did. This type of prototyping is great, because you can see what's happening."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The program <i> cut </i> is super helpful. We can parse many output streams with <i>cut</i>.\n",
    "If the format of a steam in tablular, then <i>awk</i> maybe the best, but <i>awk</i> is a whole animal into itself and I think most days, I rely on <i>cut</i> and then step into ipython use use Pandas or just Numpy.\n",
    "\n",
    "The <i>-d,</i> option is to set the delimiter and the <i>-f 3</i> says to use the 3rd field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"c\"\n",
      "\"c\"\n",
      "\"c\"\n",
      "\"c\"\n",
      "\"NONE\"\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "root_dir=\"/home/daniel/git/Python2.7/DataScience/command_line_pres_data\"\n",
    "cat $root_dir/linux_inventory.csv | cut -d, -f 3 | tail -n 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"y\"\n",
      "\"y\"\n",
      "\"y\"\n",
      "\"y\"\n",
      "\"ymfsb\"\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "root_dir=\"/home/daniel/git/Python2.7/DataScience/command_line_pres_data\"\n",
    "cat \"${root_dir}/linux_inventory.csv\" | cut -d, -f 3 | sort  | tail -n  5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      1 \"x86\"\n",
      "    105 \"xml\"\n",
      "      6 \"xsl\"\n",
      "      5 \"y\"\n",
      "      1 \"ymfsb\"\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "root_dir=\"/home/daniel/git/Python2.7/DataScience/command_line_pres_data\"\n",
    "cat \"${root_dir}/linux_inventory.csv\" | cut -d, -f 3 | sort | uniq -c | tail -n 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    849 \"txt\"\n",
      "   1080 \"S\"\n",
      "   2391 \"NONE\"\n",
      "  11622 \"h\"\n",
      "  13147 \"c\"\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "root_dir=\"/home/daniel/git/Python2.7/DataScience/command_line_pres_data\"\n",
    "cat $root_dir/linux_inventory.csv | cut -d, -f 3 | sort | uniq -ic | sort -n | tail -n 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting this command line data into a database.\n",
    "* Pandas:  df.to_sql()\n",
    "* csvkit: csvsql\n",
    "\n",
    "### Both Pandas and csvkit use SqlAlchemy to handle the connection to the DB. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "root=\"/home/daniel/git/Python2.7/DataScience/command_line_pres_data\"\n",
    "# I'm not expert at SqlAlchemy, but learning the connection strings are quite important\n",
    "# That's the, \"mysql://<log-in>:<passwd>@<ip>/<db-name>\"  I'm running mySQL on locally, localhost has IP 127.0.0.1\n",
    "csvsql --db \"mysql://root:test@127.0.0.1/LinuxKernel\" --table \"inventory\" --insert \"${root}/linux_inventory.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check that it worked\n",
    "\n",
    "The notebook isn't using the alias the way I expected so I had to type out the full command with user and password.\n",
    "In normal practice you'd make an alias as in the comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path\tisText\text\ttype\tsize\tsloc\tcomments\tblank\ttot_lines\n",
      "linux-2.6.32.67/arch/h8300/mm/fault.c\t1\tc\tC source ASCII text\t1441\t23\t28\t6\t57\n",
      "linux-2.6.32.67/arch/h8300/mm/kmap.c\t1\tc\tC source ASCII text\t1276\t27\t25\t8\t60\n",
      "linux-2.6.32.67/arch/h8300/mm/Makefile\t1\tNULL\tASCII text\t114\t0\t0\t0\t5\n",
      "linux-2.6.32.67/arch/h8300/mm/init.c\t1\tc\tC source ASCII text\t5463\t120\t56\t27\t201\n",
      "linux-2.6.32.67/arch/h8300/mm/memory.c\t1\tc\tC source ASCII text\t1099\t26\t21\t9\t56\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "#alias mysql='mysql --user=root --password=test'\n",
    "#mysql -e \"SELECT * FROM inventory LIMIT 5;\" LinuxKernel\n",
    "\n",
    "mysql --user=root --password=test -e \"SELECT * FROM inventory LIMIT 5;\" LinuxKernel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dts,115\n",
      "txt,849\n",
      "S,1080\n",
      "h,11623\n",
      "c,13147\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "sql2csv --db mysql://root:test@127.0.0.1/LinuxKernel --query \"select ext, count(ext) from inventory group by ext order by count(ext);\" | tail -n 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The numbers checkout.\n",
    "\n",
    "#### Lets explore <i>awk</i> a little. The easiest thing to make do (and perhaps the most useful), is to print columns or rows from a file stream.\n",
    "\n",
    "<i>awk</i>\n",
    "* FS field separator set to comma\n",
    "* $3 is the 3rd column\n",
    "\n",
    "<i>grep</i>\n",
    "* -E regular expression\n",
    "* -c count occurance (grep operates per line, we use pcregrep for multiple line searches)\n",
    "\n",
    "There is a way to do the whole regex and count in <i>awk</i> I just didn't want to get into it. I'm still learning <i>awk</i> myself, and I haven't decided on it's usefulness over other tools. If I'm already in a database then forget it...I mostly want to show use of <i>grep</i> and <i>awk</i> for column parsing ( rather than cut )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1699\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "root_dir=\"/home/daniel/git/Python2.7/DataScience/command_line_pres_data\"\n",
    "\n",
    "cat \"${root_dir}/linux_inventory.csv\"  | awk  'FS=\",\"; {print $3}' | grep -cE \"txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ext,count(ext)\n",
      "txt,2491\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "root_dir=\"/home/daniel/git/Python2.7/DataScience/command_line_data\"\n",
    "\n",
    "sql2csv --db mysql://root:test@127.0.0.1/LinuxKernel --query \"select ext, count(ext) from inventory where ext='txt';\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Break down a line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# do not run\n",
    "find \"${root_dir}/linux-2.6.32.67\" -maxdepth 2 -mindepth 1 -type f -iname \"*.c\" | \\\n",
    "    xargs -n 1 pcregrep -no \"(?sim)[a-z]+\\w*\\(.*?\\)\" /dev/null | \\\n",
    "    sed 's/\\s//g' | \\\n",
    "    tail -n 10 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## find ${root_dir}/linux-2.6.32.67\" -maxdepth 2 -mindepth 1 -type f -iname \"*.c\"\n",
    "\n",
    "* path to search\n",
    "* -maxdepth 2 dont', search past 2 directories deep\n",
    "* -mindepth 1, search at least 1 directory deep   (using this to save time and the notebook kept crashing)\n",
    "* -type f, only look for files not directories or links or anything else\n",
    "* -iname,  case insensitive glob style name matching, just like <i>ls</i> uses\n",
    "\n",
    "## xargs\n",
    "A neat helper program, that takes the output from another shell program, and parses it into discrete intput arguments for the next program in a pipe.\n",
    "\n",
    "This will allow us to pass one line at a time from find, to grep. Some programs do not need xargs, as they are designed to take a stream of input. Not in this case however.\n",
    "\n",
    "## pcregrep -no \"(?sim)[a-z]+\\w*\\(.*?\\)\" /dev/null\n",
    "We also introduct, <i>Pearl Compatiple Regular Expression Grep (pcregrep)</i>. \n",
    "The <i>(?sim)</i> are options:\n",
    "\n",
    "* s dot matches everything including newlines \"\\n\"\n",
    "* i case insensitive\n",
    "* m multiline\n",
    "\n",
    "The <i>(.*?)</i> makes the greedy \".\" stop, after encountering a left parenthesis, escaped like this \\\\( \n",
    "This page explains the \"Lazy Trap\" issue, where the .*? contron on the greedy match \"jumps the fence\"\n",
    "http://www.rexegg.com/regex-quantifiers.html#lazytrap\n",
    "\n",
    "### This is going to hunt for functions\n",
    "That is, strings that match (?sim)[a-z]+\\w*\\(.*?\\), where:\n",
    "\n",
    "* [a-z] a list of lower case letters\n",
    "* '+' at least once, or more\n",
    "* \\w any alpha-numeric zero or more times\n",
    "* \\( literal left parenthesis\n",
    "* greedy match until right paren\n",
    "\n",
    "The <i>/dev/null</i> is a trick to make grep print the file path it's working on.\n",
    "\n",
    "## sed \n",
    "used to remove any spaces:\n",
    "sed 's/\\s//g'\n",
    "\n",
    "* s  substitute\n",
    "* \\s  regular expression for any kind of while space\n",
    "* //  replace with nothing....easier to read if it was, sed 's/foo/bar/g' , replace 'foo' with 'bar'\n",
    "* g globally, as many times as a match can be made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/daniel/git/Python2.7/DataScience/command_line_pres_data/linux-2.6.32.67/fs/dcookies.c:332:mutex_lock(&dcookie_mutex)\n",
      "/home/daniel/git/Python2.7/DataScience/command_line_pres_data/linux-2.6.32.67/fs/dcookies.c:334:list_del(&user->next)\n",
      "/home/daniel/git/Python2.7/DataScience/command_line_pres_data/linux-2.6.32.67/fs/dcookies.c:335:kfree(user)\n",
      "/home/daniel/git/Python2.7/DataScience/command_line_pres_data/linux-2.6.32.67/fs/dcookies.c:337:is_live()\n",
      "/home/daniel/git/Python2.7/DataScience/command_line_pres_data/linux-2.6.32.67/fs/dcookies.c:338:dcookie_exit()\n",
      "/home/daniel/git/Python2.7/DataScience/command_line_pres_data/linux-2.6.32.67/fs/dcookies.c:340:mutex_unlock(&dcookie_mutex)\n",
      "/home/daniel/git/Python2.7/DataScience/command_line_pres_data/linux-2.6.32.67/fs/dcookies.c:343:EXPORT_SYMBOL_GPL(dcookie_register)\n",
      "/home/daniel/git/Python2.7/DataScience/command_line_pres_data/linux-2.6.32.67/fs/dcookies.c:344:EXPORT_SYMBOL_GPL(dcookie_unregister)\n",
      "/home/daniel/git/Python2.7/DataScience/command_line_pres_data/linux-2.6.32.67/fs/dcookies.c:345:EXPORT_SYMBOL_GPL(get_dcookie)\n",
      "/home/daniel/git/Python2.7/DataScience/command_line_pres_data/linux-2.6.32.67/fs/no-block.c:15:no_blkdev_open(structinode*inode,structfile*filp)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "root_dir=\"/home/daniel/git/Python2.7/DataScience/command_line_pres_data\"\n",
    "\n",
    "find \"${root_dir}/linux-2.6.32.67\" -maxdepth 2 -mindepth 1 -type f -iname \"*.c\" | xargs -n 1 pcregrep -no \"(?sim)[a-z]+\\w*\\(.*?\\)\" /dev/null | sed 's/\\s//g' | tail -n 10 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This example used the regular expression syntax where as above, I used the \"globbing\" rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/daniel/git/Python2.7/DataScience/command_line_pres_data/linux-2.6.32.67/fs/dcookies.c:332:mutex_lock(&dcookie_mutex)\n",
      "/home/daniel/git/Python2.7/DataScience/command_line_pres_data/linux-2.6.32.67/fs/dcookies.c:334:list_del(&user->next)\n",
      "/home/daniel/git/Python2.7/DataScience/command_line_pres_data/linux-2.6.32.67/fs/dcookies.c:335:kfree(user)\n",
      "/home/daniel/git/Python2.7/DataScience/command_line_pres_data/linux-2.6.32.67/fs/dcookies.c:337:is_live()\n",
      "/home/daniel/git/Python2.7/DataScience/command_line_pres_data/linux-2.6.32.67/fs/dcookies.c:338:dcookie_exit()\n",
      "/home/daniel/git/Python2.7/DataScience/command_line_pres_data/linux-2.6.32.67/fs/dcookies.c:340:mutex_unlock(&dcookie_mutex)\n",
      "/home/daniel/git/Python2.7/DataScience/command_line_pres_data/linux-2.6.32.67/fs/dcookies.c:343:EXPORT_SYMBOL_GPL(dcookie_register)\n",
      "/home/daniel/git/Python2.7/DataScience/command_line_pres_data/linux-2.6.32.67/fs/dcookies.c:344:EXPORT_SYMBOL_GPL(dcookie_unregister)\n",
      "/home/daniel/git/Python2.7/DataScience/command_line_pres_data/linux-2.6.32.67/fs/dcookies.c:345:EXPORT_SYMBOL_GPL(get_dcookie)\n",
      "/home/daniel/git/Python2.7/DataScience/command_line_pres_data/linux-2.6.32.67/fs/no-block.c:15:no_blkdev_open(structinode*inode,structfile*filp)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "root_dir=\"/home/daniel/git/Python2.7/DataScience/command_line_pres_data\"\n",
    "find \"${root_dir}/linux-2.6.32.67\" -maxdepth 2 -mindepth 1 -type f -regextype posix-extended -regex \".*\\.(c|h|cpp)\" | \\\n",
    "    xargs -n 1 pcregrep  -no \"(?sim)[a-z]+\\w*\\(.*?\\)\" /dev/null | sed 's/\\s//g' | tail -n 10 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Aside about loops vs the Parallel program\n",
    "## While loops in bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process is interrupted.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "root_dir=\"/home/daniel/git/Python2.7/DataScience/command_line_pres_data\"\n",
    "\n",
    "while read f;do   \n",
    "    file_=$(echo $f | cut -d, -f 1 | sed 's/\\\"//g')\n",
    "    grep -Eo -m 1 \"^#include <linux\"  \"${file_}\" /dev/null\n",
    "done < \"${root_dir}/linux_inventory.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root_dir=\"/home/daniel/git/Python2.7/DataScience/command_line_pres_data\"\n",
      "\n",
      "input_=$1\n",
      "file_=$(echo $input_ | cut -d, -f 1 | sed s/\\\"//g)\n",
      "out=$(grep -Eo -m 1 \"^#include <linux\"  \"${root_dir}/${file_}\" /dev/null)\n",
      "echo $out\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "root_dir=\"/home/daniel/git/Python2.7/DataScience/command_line_pres_data\"\n",
    "\n",
    "cat $root_dir/prep.sh "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/daniel/git/Python2.7/DataScience/command_line_pres_data/linux-2.6.32.67/drivers/rapidio/rio.c:#include <linux\n",
      "/home/daniel/git/Python2.7/DataScience/command_line_pres_data/linux-2.6.32.67/drivers/rapidio/switches/tsi500.c:#include <linux\n",
      "/home/daniel/git/Python2.7/DataScience/command_line_pres_data/linux-2.6.32.67/drivers/rapidio/rio.h:#include <linux\n",
      "\n",
      "/home/daniel/git/Python2.7/DataScience/command_line_pres_data/linux-2.6.32.67/drivers/rapidio/rio-access.c:#include <linux\n",
      "/home/daniel/git/Python2.7/DataScience/command_line_pres_data/linux-2.6.32.67/drivers/rapidio/rio-scan.c:#include <linux\n",
      "\n",
      "/home/daniel/git/Python2.7/DataScience/command_line_pres_data/linux-2.6.32.67/drivers/rapidio/rio-sysfs.c:#include <linux\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "root_dir=\"/home/daniel/git/Python2.7/DataScience/command_line_pres_data\"\n",
    "\n",
    "tail -n 10 \"${root_dir}/linux_inventory.csv\" | xargs -P 4 -n 1 $root_dir/prep.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <i>Parallel</i>\n",
    "\n",
    "A powerful program that makes running shell programs on multiple cores trivial, is <i>parallel</i> .\n",
    "It also cleans up code and makes things a single line when used with just a single core."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "real\t0m2.031s\n",
      "user\t0m0.828s\n",
      "sys\t0m1.250s\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Free the cache to really test the timing\n",
    "# become root and run \n",
    "# free && sync && echo 3 > /proc/sys/vm/drop_caches && free\n",
    "\n",
    "root_dir=\"/home/daniel/git/Python2.7/DataScience/command_line_pres_data\"\n",
    "time find \"${root_dir}/linux-2.6.32.67\" -maxdepth 2 -mindepth 1 -type f -name \"*.c\" | \\\n",
    "    parallel --jobs 1 -n 1 'pcregrep -no \"(?sm)if\\s*\\(.*?\\)\" /dev/null' | sed 's/\\s//g' > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "find: `/linux-2.6.32.67': No such file or directory\n",
      "\n",
      "real\t0m0.121s\n",
      "user\t0m0.056s\n",
      "sys\t0m0.043s\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# become root and run \n",
    "# free && sync && echo 3 > /proc/sys/vm/drop_caches && free\n",
    "time find \"${root_dir}/linux-2.6.32.67\" -maxdepth 2 -mindepth 1 -type f -name \"*.c\" | \\\n",
    "    parallel --jobs 4 -n 1 'pcregrep -no \"(?sm)if\\s*\\(.*?\\)\" /dev/null' | sed 's/\\s//g' > /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add columns to our inventory database\n",
    "Let's say that we want to add a categorical columns to our SQL table. This is a nice way to store information about a table without wororying about normalization. We are not DBA's trying to maintain strict schema here. We just need a fast and intuitive way to query our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Categorical Variables\n",
    "I had some difficulty when I first started reading about categorical variables. They are writen and talked about in various contexts. For now, I'm thinking about a column in a database like table, that contains a string, which describes the row.\n",
    "\n",
    "In contrast, I could have had a boolean flag like columns. For every file with an include statement that has \"linux\" in it, give it a 1 and the rest a 0. Then for another condition, I'd have to add a flag for it, \"foo\" with 1 and 0's and so on. \n",
    "\n",
    "In this case, a categorical variable is a single column that makes it wasy to do \"group by\", and aggregrate across categories. We can use more logic, such as \"where\" clauses, to define even more granular categories."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "So let's add a column for the presense of the include statement, \"#include <linux/*>\" ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "root_dir=\"/home/daniel/git/Python2.7/DataScience/command_line_pres_data\"\n",
    "\n",
    "find \"${root_dir}/linux-2.6.32.67\" -regextype posix-extended -regex \".*\\.(c|h)\" -type f | \\\n",
    "    parallel --jobs 4 -n 1 'grep -Eo -m 1 \"^#include\\s\\Wlinux/\\w+\\.h\\W\" /dev/null' > \"${root_dir}/sub_list.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "root_dir=\"/home/daniel/git/Python2.7/DataScience/command_line_pres_data\"\n",
    "\n",
    "csvsql --db \"mysql://root:test@127.0.0.1/LinuxKernel\" --table \"inventory\" --insert \"${root_dir}/linux_inventory.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use sed ( stream editor ) to add the header row. You can certainly open the file in an editor and do this manually. I like to practice sed as much as possible b/c it takes a while to learn . I often use this synatx, \n",
    "<i>sed 's/foo/bar/g' to substitue.</i> \n",
    "\n",
    "However, this a an \"address\" command and to be honest I've been forgetting how to use it, but that's what this Meetup is all about, practice through teaching and sharing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "root_dir=\"/home/daniel/git/Python2.7/DataScience/command_line_pres_data\"\n",
    "\n",
    "sed -i '1 a\\path_:include' \"${root_dir}/sub_list.txt\" # address command, 'append' at first line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "root_dir=\"/home/daniel/git/Python2.7/DataScience/command_line_pres_data\"\n",
    "\n",
    "csvsql -d \":\" --db \"mysql://root:test@127.0.0.1/LinuxKernel\" --table \"sub_list\" --insert \"${root_dir}/sub_list.txt\"  "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "update inventory set category = \n",
    "       case when inventory.path in \n",
    "\t     (select path_ from sub_list)\n",
    "       then \"linux\" else NULL\n",
    " end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sql=\"update inventory set category = case when inventory.path in (select path_ from sub_list) then 'linux' else NULL end;\"\n",
    "\n",
    "mysql -e \"${sql}\" LinuxKernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "sql2csv --db \"mysql://root:test@127.0.0.1/LinuxKernel\" --table \"inventory\" --query \"SELECT * from inventory;\" > linux_inventory_include.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "root_dir=\"/home/daniel/git/Python2.7/DataScience/command_line_pres_data\"\n",
    "\n",
    "\n",
    "find \"${root_dir}/linux-2.6.32.67\" -regextype posix-extended -regex \".*\\.(c|h)\" -type f | \\\n",
    "    parallel --jobs 4 -n 1 'grep -Eo -m 1 \"^#include\\s\\Wlinux/kernel\\.h\\W\" /dev/null' > linux_inventory_kernel.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do the same thing with Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sqlalchemy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import path\n",
    "root_dir = \"/home/daniel/git/Python2.7/DataScience/command_line_pres_data\"\n",
    "engine = sqlalchemy.create_engine(\"mysql://root:test@127.0.0.1/LinuxKernel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What if we did the find | grep steps in  Python....? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "ob = re.compile(\"#include\\s\\Wlinux/(?P<name>\\w+)\\.h\\W\")\n",
    "\n",
    "def regex_check(filename):\n",
    "    fname = path.join(root_dir, filename)\n",
    "    f = open(fname, 'r')\n",
    "    match = ob.search(f.read())\n",
    "    f.close()\n",
    "    \n",
    "    if match:\n",
    "        return match.group('name')\n",
    "    \n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rio\n"
     ]
    }
   ],
   "source": [
    "print regex_check('linux-2.6.32.67/drivers/rapidio/rio-access.c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "src = path.join(root_dir, \"linux_inventory.csv\")\n",
    "df = pd.read_csv(src, quotechar='\"', quoting=1)\n",
    "df['category'] = df.apply(lambda row: regex_check(row['path']) if row['ext'] == 'h' or row['ext'] == 'c' else None, axis=1)\n",
    "#df.to_sql(\"inventory2\", engine, flavor='mysql')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          mutex\n",
       "1             if\n",
       "2         kernel\n",
       "3    etherdevice\n",
       "4            err\n",
       "5         bitops\n",
       "6           None\n",
       "7         kernel\n",
       "8         device\n",
       "9           None\n",
       "Name: category, dtype: object"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['category'][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.to_sql(\"inventory2\", engine, flavor='mysql')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cts\tcategory\n",
      "15235\tNULL\n",
      "3002\tmodule\n",
      "2266\tkernel\n",
      "1534\ttypes\n",
      "1327\tinit\n",
      "319\tdelay\n",
      "318\terrno\n",
      "259\tfs\n",
      "245\tsched\n",
      "241\tmm\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "sql=\"select count(path) as cts, category from inventory2 group by category order by cts desc limit 10;\"\n",
    "mysql -uroot -ptest -e \"${sql}\" LinuxKernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "root_dir=\"/home/daniel/git/Python2.7/DataScience/command_line_pres_data\"\n",
    "\n",
    "sql2csv --db \"mysql://root:test@127.0.0.1/LinuxKernel\" --query \"select count(path) as cts, category from inventory2 group by category order by cts desc;\" \\\n",
    "> \"${root_dir}/categories.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
